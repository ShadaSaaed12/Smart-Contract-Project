# ğŸ“„ Smart Contract Summary & Q&A Assistant

## ğŸ“Œ Project Overview

Smart Contract Summary & Q&A Assistant is a Retrieval-Augmented Generation (RAG) based web application that allows users to upload contract documents (PDF/DOCX) and interact with them through a conversational interface.

The system extracts text from uploaded documents, processes and embeds the content into a vector database, and enables users to ask questions grounded in the uploaded document. The assistant generates answers with contextual awareness based only on the provided file.

---

## ğŸ¯ Project Objectives

- Demonstrate understanding of LLM inference workflows  
- Build an end-to-end RAG (Retrieval-Augmented Generation) pipeline  
- Implement document ingestion and semantic search  
- Integrate vector databases for similarity retrieval  
- Expose the system as an API using LangServe  
- Provide a user-friendly interface using Gradio  

---

## ğŸ— System Architecture

User â†’ Gradio UI â†’ LangServe API â†’ RetrievalQA Chain â†’ Vector Store â†’ LLM â†’ Response  

### ğŸ”¹ Main Components

- Frontend: Gradio (Upload & Chat Interface)  
- Backend: FastAPI + LangServe  
- Pipeline: Document ingestion + RetrievalQA  
- Vector Store: Chroma (or FAISS)  
- LLM: OpenAI Chat Model  
- Embeddings: OpenAI Embeddings  

---

## âš™ï¸ Technologies Used

- Python  
- LangChain  
- LangServe  
- FastAPI  
- Gradio  
- Chroma (Vector Database)  
- OpenAI API  
- PyPDF / document loaders  

---

## ğŸ”„ How the System Works

1ï¸âƒ£ User uploads a contract (PDF)  
2ï¸âƒ£ The document is parsed and converted to text  
3ï¸âƒ£ Text is split into smaller chunks  
4ï¸âƒ£ Chunks are converted into embeddings  
5ï¸âƒ£ Embeddings are stored in a vector database  
6ï¸âƒ£ User submits a question  
7ï¸âƒ£ The system retrieves the most relevant document chunks  
8ï¸âƒ£ The LLM generates an answer based on retrieved context  
9ï¸âƒ£ The response is displayed in the chat interface  

---

## ğŸ“‚ Project Structure
smart_contract_assistant/
â”‚
â”œâ”€â”€ ingestion.py      # Handles PDF loading, chunking, and embedding
â”œâ”€â”€ chain.py          # Builds RetrievalQA chain
â”œâ”€â”€ server.py         # Exposes the chain via LangServe API
â”œâ”€â”€ ui.py             # Gradio user interface
â”œâ”€â”€ requirements.txt  # Dependencies
â””â”€â”€ README.md         # Project documentation

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install Dependencies
pip install langchain langserve fastapi uvicorn gradio chromadb pypdf openai

---

### 2ï¸âƒ£ Set OpenAI API Key (Windows)
setx OPENAI_API_KEY "your_api_key_here"

Restart terminal after setting the key.

---

### 3ï¸âƒ£ Run Ingestion

Place your contract file in the project folder and run:
python run_ingest.py

This creates the vector database.

---

### 4ï¸âƒ£ Start Backend Server
python server.py

---

### 5ï¸âƒ£ Launch Gradio UI

Open a new terminal:
python ui.py

Open the provided link in your browser.

---

## ğŸ“Š Evaluation

### ğŸ“ˆ Metrics Considered

- Relevance of retrieved chunks  
- Answer grounding in document  
- Response latency (< 5 seconds for medium contracts)  

### âš ï¸ Limitations

- Supports English documents only  
- Depends on OpenAI API availability  
- Large documents may increase processing time  
- Not production-grade deployment  

---

## ğŸš€ Future Enhancements

- Multi-document support  
- Local LLM integration  
- Role-based access control  
- Cloud deployment (Docker/Kubernetes)  
- Improved evaluation metrics
